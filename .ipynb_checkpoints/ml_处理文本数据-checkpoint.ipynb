{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "martial-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本通常只是数据集中的字符串，但并非所有字符串特征都应该被当做文本来处理，常见四种类型\n",
    "# 分类数据、可以在语义上映射为类别的自由字符串、结构化字符串数据、文本数据\n",
    "# 在文本分析的语境中，数据集通常被称为语料库，每个由单个文本表示的数据点被称为文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "governing-least",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpye of text_train:<class 'list'>\n",
      "length of text_train:75000\n",
      "text_train[1]:\n",
      "b\"Amount of disappointment I am getting these days seeing movies like Partner, Jhoom Barabar and now, Heyy Babyy is gonna end my habit of seeing first day shows.<br /><br />The movie is an utter disappointment because it had the potential to become a laugh riot only if the d\\xc3\\xa9butant director, Sajid Khan hadn't tried too many things. Only saving grace in the movie were the last thirty minutes, which were seriously funny elsewhere the movie fails miserably. First half was desperately been tried to look funny but wasn't. Next 45 minutes were emotional and looked totally artificial and illogical.<br /><br />OK, when you are out for a movie like this you don't expect much logic but all the flaws tend to appear when you don't enjoy the movie and thats the case with Heyy Babyy. Acting is good but thats not enough to keep one interested.<br /><br />For the positives, you can take hot actresses, last 30 minutes, some comic scenes, good acting by the lead cast and the baby. Only problem is that these things do not come together properly to make a good movie.<br /><br />Anyways, I read somewhere that It isn't a copy of Three men and a baby but I think it would have been better if it was.\"\n"
     ]
    }
   ],
   "source": [
    "# 示例应用：电影评论的情感分析\n",
    "# 使用aclimdb数据集，数据集包含评论文本，及一个标签用于表示该评论是“正面的”还是“负面的”\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "# load_files返回一个Bunch对象，其中包含训练文本和训练标签\n",
    "reviews_train = load_files(\"./data/aclimdb/train/\")\n",
    "text_trian, y_train = reviews_train.data, reviews_train.target\n",
    "print(\"tpye of text_train:{}\".format(type(text_trian)))\n",
    "print(\"length of text_train:{}\".format(len(text_trian)))\n",
    "print(\"text_train[1]:\\n{}\".format(text_trian[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "patient-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清洗文本中存在的HTML换行符\n",
    "text_trian = [doc.replace(b\"<br />\", b\" \") for doc in text_trian]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pediatric-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤掉标签为2的数据\n",
    "import pandas as pd\n",
    "def filter_data(X, y):\n",
    "    df = pd.DataFrame({'data': text_trian,\n",
    "                          'class': y_train})\n",
    "    df_new = df[df['class']!=2]\n",
    "    return df_new['data'], df_new['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conditional-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_trian, y_train = filter_data(text_trian, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "enclosed-tonight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "# 加载测试数据\n",
    "import numpy as np\n",
    "\n",
    "reviews_test = load_files(\"./data/aclimdb/test\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "weird-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本数据并不是机器学习可以处理的格式，我们需要将文本的字符串表示转换为数值表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "valid-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本数据表示为词袋 是用于机器学习的文本表示的一种最简单也是最有效且常用的方法\n",
    "# 使用这种表示时我们舍弃文本的机构，只计算语料库中每个单词在每个文本中出现的频次\n",
    "# 对于文档语料库，计算词袋表示包括以下三个步骤：\n",
    "# ①分词：将每个文档划分为出现在其中的单词(成为词例)，比如按空格和标点划分\n",
    "# ②构建词表：收集一个词表，里面包含出现在任意文档中的所有词，并进行编号(比如按字母排序)\n",
    "# ③编码：对于每个文档，计算词表中每个单词在该文档中出现的频次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "three-internet",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:13\n",
      "Vocabulary content:\n",
      "{'himself': 5, 'think': 10, 'to': 11, 'the': 9, 'man': 8, 'be': 0, 'wise': 12, 'he': 4, 'knows': 7, 'is': 6, 'but': 1, 'doth': 2, 'fool': 3}\n"
     ]
    }
   ],
   "source": [
    "# 词袋表示是在CountVectorizer中实现，它是一个转换器(transfromer)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bards_words = [\"The fool doth think he is wise,\",\n",
    "               \"but the wise man knows himself to be a fool\"]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)\n",
    "# 可通过vocabulary_属性来访问词表\n",
    "print(\"Vocabulary size:{}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n{}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "center-attraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words:<2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# 创建训练数据的词袋表示\n",
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"bag_of_words:{}\".format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "elect-pride",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense representation of bag_of_words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# 词袋表示保存在一个Scipy稀疏矩阵中，这种格式只保存非零元素，每个特征对应词表中一个单词\n",
    "# 查看稀疏矩阵的实际内容，可使用toarray方法将其转换为“密集的”NumPy数组(保存所有0元素)\n",
    "# 但是对于真实数据将会导致MemoryError(内存错误)\n",
    "print(\"Dense representation of bag_of_words:\\n{}\".format(bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "secure-journal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# 将词袋应用于电影评论数据\n",
    "vect = CountVectorizer().fit(text_trian)\n",
    "X_train = vect.transform(text_trian)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "automotive-bangkok",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:74849\n",
      "First 20 features:\n",
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']\n",
      "Features 20010 to 20030:\n",
      "['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl', 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\n",
      "Every 2000th feature:\n",
      "['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery', 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer', 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful', 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher', 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse', 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\n"
     ]
    }
   ],
   "source": [
    "# 访问词表的还可使用向量器的get_feature_name方法它将返回一个列表,每个元素对应一个特征\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features:{}\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "apparent-weekend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy:0.88\n"
     ]
    }
   ],
   "source": [
    "# 尝试在未改进特征提取的数据上进行分类训练，对高维稀疏数据，类似逻辑回归的线性模型效果好\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train, y_train, \n",
    "                                                cv=5)\n",
    "print(\"Mean cross-validation accuracy:{:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "false-whole",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score:0.89\n",
      "Besr parameters: {'C': 0.1, 'max_iter': 1000}\n"
     ]
    }
   ],
   "source": [
    "# 尝试使用网格搜索交叉验证来调节正则化参数C\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parm_grid = {'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "            'max_iter':[1000]}\n",
    "grid = GridSearchCV(LogisticRegression(), parm_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score:{:.2f}\".format(grid.best_score_))\n",
    "print(\"Besr parameters:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fantastic-acquisition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set score:0.88\n"
     ]
    }
   ],
   "source": [
    "# 在测试集上评估\n",
    "X_test = vect.transform(text_test)\n",
    "print(\"Test-set score:{:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "latter-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试改进单次提取\n",
    "# CountVectorizer使用正则表达式提取词例，默认使用的正则表达式是“\\b\\w\\w+\\b”，然后会\n",
    "# 将所有的单词转换为小写字母；前面我们还得到了许多不包含任何信息量的特征(比如数字),\n",
    "# 减少这些特征的一种方法就是设置仅使用至少在n个文档中出现过的词例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "encouraging-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df:<25000x27271 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3354014 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# min_df参数设置词例至少出现次数\n",
    "vect = CountVectorizer(min_df=5).fit(text_trian)\n",
    "X_train = vect.transform(text_trian)\n",
    "print(\"X_train with min_df:{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "silver-albania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 feature:\n",
      "['00', '000', '007', '00s', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '100', '1000', '100th', '101', '102', '103', '104', '105', '107', '108', '10s', '10th', '11', '110', '112', '116', '117', '11th', '12', '120', '12th', '13', '135', '13th', '14', '140', '14th', '15', '150', '15th', '16', '160', '1600', '16mm', '16s', '16th']\n",
      "Features 20010 to 20030:\n",
      "['repentance', 'repercussions', 'repertoire', 'repetition', 'repetitions', 'repetitious', 'repetitive', 'rephrase', 'replace', 'replaced', 'replacement', 'replaces', 'replacing', 'replay', 'replayable', 'replayed', 'replaying', 'replays', 'replete', 'replica']\n",
      "Every 700th feature:\n",
      "['00', 'affections', 'appropriately', 'barbra', 'blurbs', 'butchered', 'cheese', 'commitment', 'courts', 'deconstructed', 'disgraceful', 'dvds', 'eschews', 'fell', 'freezer', 'goriest', 'hauser', 'hungary', 'insinuate', 'juggle', 'leering', 'maelstrom', 'messiah', 'music', 'occasional', 'parking', 'pleasantville', 'pronunciation', 'recipient', 'reviews', 'sas', 'shea', 'sneers', 'steiger', 'swastika', 'thrusting', 'tvs', 'vampyre', 'westerns']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print(\"First 50 feature:\\n{}\".format(feature_names[:50]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 700th feature:\\n{}\".format(feature_names[::700]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fatal-county",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score:0.89\n",
      "Besr parameters: {'C': 0.1, 'max_iter': 1000}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), parm_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score:{:.2f}\".format(grid.best_score_))\n",
    "print(\"Besr parameters:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "israeli-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 虽然网格搜索最佳验证的精度没有提升，但减少要处理的特征数量可以加速处理过程\n",
    "# 舍弃无用特征也可能提高模型的可解释性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "confirmed-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词 删除没有信息量的单词还有另一种方法，就是舍弃那些出现次数太多以至于没有信息量的\n",
    "# 单词，有两种主要方法：使用特定语言的停用词列表，或者舍弃那些出现过于频繁的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "pediatric-shirt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of step words:318\n",
      "Every 10th stopword:\n",
      "['made', 'something', 'until', 'via', 'to', 'keep', 'un', 'throughout', 'out', 'whole', 'hence', 'least', 'cant', 'same', 'thin', 'only', 'herein', 'two', 'please', 'yet', 'amount', 'several', 'by', 'when', 'too', 'on', 'others', 'becoming', 'amoungst', 'at', 'thence', 'find']\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn的feature_extraction.text模块中提供了英语停用词的内置列表\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print(\"Number of step words:{}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "separate-court",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with stop words:\n",
      "<25000x26445 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2120667 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# 指定stop_words=\"english\"将使用内置列表，我们也可以扩展这个列表并传入我们自己的列表\n",
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_test)\n",
    "X_train = vect.transform(text_trian)\n",
    "print(\"X_train with stop words:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "looking-computer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score:0.88\n"
     ]
    }
   ],
   "source": [
    "# 特征数量比前面减少了305个，说明大部分停用词都出现了\n",
    "grid = GridSearchCV(LogisticRegression(), parm_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score:{:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "furnished-supervision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with stop words:\n",
      "<25000x26709 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2711464 stored elements in Compressed Sparse Row format>\n",
      "Best cross-validation score:0.89\n"
     ]
    }
   ],
   "source": [
    "# 使用停用词后的网格搜索性能略有下降，但鉴于从27000多个特征中删除305个不太可能对性能\n",
    "# 和解释性造成很大影响，所以使用这个列表似乎是不值得，固定列表主要对小型数据集很有帮助，\n",
    "# 这些数据集可能没有包含足够的信息，模型从数据本身无法判断哪些单词是停用词\n",
    "# 可以尝试另一种方法，通过设置max_df参数来舍弃高频单词\n",
    "vect = CountVectorizer(min_df=5, max_df=10000).fit(text_test)\n",
    "X_train = vect.transform(text_trian)\n",
    "print(\"X_train with stop words:\\n{}\".format(repr(X_train)))\n",
    "grid = GridSearchCV(LogisticRegression(), parm_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score:{:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "offshore-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词频-逆文档频率(tf-idf) 是一种按照我们预计的信息量大小老缩放特征，而不是舍弃那些认为\n",
    "# 不重要的特征，这一方法对某个特定文档中将常出现的术语给予很高的权重，但对在语料库中的\n",
    "# 许多文档中都经常出现的术语给予的权重却不高\n",
    "# scikit-learn在两个类中实现了tf-idf方法：TfidfTransformer和TfidfVectorizer\n",
    "# TfidfTransfromer接受CountVectorizer生成的稀疏矩阵并将其变换\n",
    "# TfidfVectorizer接受文本数据并完成词袋特征提取与tf-idf变换\n",
    "# 单词w在文档d中的tf-idf分数计算公式：tfinf(w,d) = tf log ((N+1) / (Nw+1)) + 1\n",
    "# 其中N是训练集中的文档数量，Nw是训练集中出现单词w的文档数量，tf(词频)是单词w在文档d中\n",
    "# 出现的次数，这两个类在完成tf-idf表示后还应用了L2范数进行缩放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "structural-merchandise",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score:0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pyenv\\ml3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_trian, y_train)\n",
    "print(\"Best cross-validation score:{:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "weekly-alignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['suplexes' 'gauche' 'hypocrites' 'oncoming' 'songwriting' 'galadriel'\n",
      " 'emerald' 'mclaughlin' 'sylvain' 'oversee' 'cataclysmic' 'pressuring'\n",
      " 'uphold' 'thieving' 'inconsiderate' 'ware' 'denim' 'reverting' 'booed'\n",
      " 'spacious']\n",
      "Features with highest tfidf:\n",
      "['gadget' 'sucks' 'zatoichi' 'demons' 'lennon' 'bye' 'dev' 'weller'\n",
      " 'sasquatch' 'botched' 'xica' 'darkman' 'woo' 'casper' 'doodlebops'\n",
      " 'smallville' 'wei' 'scanners' 'steve' 'pokemon']\n"
     ]
    }
   ],
   "source": [
    "# 我们可以查看tf-idf找到的最重要的单词，tf-idf缩放的目的是找到能够区分文档的单词，但是\n",
    "# 它完全是一种无监督技术，因此这里的“重要”不一定与分类结果标签相关\n",
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "X_train = vectorizer.transform(text_trian)\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "                feature_names[sorted_by_tfidf[:20]]))\n",
    "print(\"Features with highest tfidf:\\n{}\".format(\n",
    "                feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "caring-expert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf:\n",
      "['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'have' 'one' 'be' 'film' 'are' 'you' 'all'\n",
      " 'at' 'an' 'by' 'so' 'from' 'like' 'who' 'they' 'there' 'if' 'his' 'out'\n",
      " 'just' 'about' 'he' 'or' 'has' 'what' 'some' 'good' 'can' 'more' 'when'\n",
      " 'time' 'up' 'very' 'even' 'only' 'no' 'would' 'my' 'see' 'really' 'story'\n",
      " 'which' 'well' 'had' 'me' 'than' 'much' 'their' 'get' 'were' 'other'\n",
      " 'been' 'do' 'most' 'don' 'her' 'also' 'into' 'first' 'made' 'how' 'great'\n",
      " 'because' 'will' 'people' 'make' 'way' 'could' 'we' 'bad' 'after' 'any'\n",
      " 'too' 'then' 'them' 'she' 'watch' 'think' 'acting' 'movies' 'seen' 'its'\n",
      " 'him']\n"
     ]
    }
   ],
   "source": [
    "# 我们还可以查看找到的逆行文档频率较低的单词，即出现次数很多，但被认为不那么重要的单词\n",
    "# 逆文档频率值被保存在idf_属性中\n",
    "sorted_by_tfidf = np.argsort(vectorizer.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    "            feature_names[sorted_by_tfidf[:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "unlike-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 研究模型系数  \n",
    "# 查看逻辑回归模型中最大的25个系数与最小的25个系数，左侧的负系数属于模型找到的表示\n",
    "# 负面评论的单词，右侧的正系数属于模型找到的表示正面评价的单词\n",
    "import mglearn\n",
    "\n",
    "mglearn.tools.visualize_coefficients(\n",
    "        grid.best_estimator_.named_steps[\"logisticregression\"].coef_,\n",
    "        feature_names, n_top_features=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-graph",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
